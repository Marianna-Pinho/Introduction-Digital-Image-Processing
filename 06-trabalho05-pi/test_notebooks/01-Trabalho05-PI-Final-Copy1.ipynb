{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Trabalho 05 - Introdução ao Processamento de Imagem Digital </center>\n",
    "\n",
    "**Aluna:** Marianna de Pinho Severo <br>\n",
    "**RA**: 264960 <br>\n",
    "**Professor:** Hélio Pedrini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Definir funções auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 2.1: Funções de apresentação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função utilizada para plotar todas as imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAllImages(images, color='rgb'):\n",
    "    columns = 4\n",
    "    rows = 3\n",
    "    i = 1\n",
    "    fig=plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for key in images:\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.xlabel(key)\n",
    "        if color == 'rgb':\n",
    "            plt.imshow(cv.cvtColor(images[key], cv.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(images[key],cmap='gray', vmin=0, vmax=255)\n",
    "        i = i+1\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.20, top=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para plotar os pares de imagens com as linhas de correspondências entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrespondingLines(images, matches, keypoints):\n",
    "    columns = 5\n",
    "    rows = 8\n",
    "    i = 1\n",
    "    fig=plt.figure(figsize=(15, 15))\n",
    "\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None)\n",
    "\n",
    "    for key in matches:\n",
    "        _,key1,key2,_ = key.split('_')\n",
    "\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.xlabel(key)\n",
    "\n",
    "        imMatches = cv.drawMatches(images[key1], keypoints[key1], images[key2], keypoints[key2], matches[key], None, **draw_params)\n",
    "        plt.imshow(cv.cvtColor(imMatches,cv.COLOR_BGR2RGB))\n",
    "        i = i+1\n",
    "\n",
    "    # plt.subplots_adjust(bottom=0.20, top=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 2.2: Funções para salvar imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que salva os pares de imagens, com linhas de correspondências entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCorrespondingLines(images, matches, keypoints):\n",
    "    draw_params = dict(matchColor = (0,255,0), singlePointColor = None)\n",
    "    \n",
    "    for key in matches:\n",
    "        subkey1,subkey2,subkey3,_ = key.split('_')\n",
    "        \n",
    "        imMatches = cv.drawMatches(images[subkey2], keypoints[subkey2], images[subkey3], keypoints[subkey3], matches[key], None, **draw_params)\n",
    "        cv.imwrite('output_images/'+subkey1+'/lines/'+key+'.jpg',imMatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 2.3: Funções para cálculos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que retorna apenas os pontos correspondentes cujas correspondências são maiores do que um determinado limiar. Para calcular a correspodência, normalizamos as distâncias entre pares de pontos correspondentes, deixando-as no intervalo [0,1]. Então, subtraímos esse valor de 1, de maneira que, quanto menor for a distância entre dois pontos, maior vai ser a correspondência.\n",
    "\n",
    "Como não sabemos a máxima distância possível entre dois pontos, para normalizarmos as distâncias, utilizamos a maior distância dentre as obtidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrespondingMatches(matches, corr_level=0.5):\n",
    "    if matches == []:\n",
    "        return []\n",
    "    \n",
    "    distances = [m.distance for m in matches]\n",
    "    distances = distances/np.max(distances)\n",
    "    \n",
    "    corr_values = np.ones((1,len(distances))) - distances\n",
    "    corr_indexes = np.where(corr_values >= corr_level)[1]\n",
    "    \n",
    "    corresponding = [matches[idx] for idx in corr_indexes]\n",
    "    \n",
    "    return corresponding if len(corresponding) > 0 else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3: Ler imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['building1'] = cv.imread('input_images/foto1A.jpg')\n",
    "images['building2'] = cv.imread('input_images/foto1B.jpg')\n",
    "images['lake1'] = cv.imread('input_images/foto2A.jpg')\n",
    "images['lake2'] = cv.imread('input_images/foto2B.jpg')\n",
    "images['road1'] = cv.imread('input_images/foto3B.jpg')\n",
    "images['road2'] = cv.imread('input_images/foto3A.jpg')\n",
    "images['desertroad1'] = cv.imread('input_images/foto4A.jpg')\n",
    "images['desertroad2'] = cv.imread('input_images/foto4B.jpg')\n",
    "images['field1'] = cv.imread('input_images/foto5A.jpg')\n",
    "images['field2'] = cv.imread('input_images/foto5B.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 4: Converter imagens para a escala de cinza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in images:\n",
    "    gray_images[key] = cv.cvtColor(images[key], cv.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotAllImages(gray_images, 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 5: Encontrar pontos de interesse e descritores invariantes\n",
    "\n",
    "Para encontrarmos pontos de interesse e calcularmos seus descritores, utilizaremos dois métodos, escolhidos por serem métodos não pagos. O primeiro deles é o ORB (*Oriented FAST and Rotated BRIEF*), que pode ser utilizando tanto para identificarmos os pontos de interesse como para calcularmos seus descritores. O segundo método é o BRIEF (*Binary Robust Independent Elementary Features*), que é utilizado apenas para o cálculo dos descritores. \n",
    "\n",
    "Um detalhe importante com relação ao BRIEF é que precisamos utilizar um outro algoritmo para a identificação dos pontos de interesse e então passamos estes para o BRIEF para que ele calcule os descritores. Segundo o tutorial do *Opencv*, um bom algoritmo para a identificação dos pontos de interesse seria o *Star*. Entretanto, ao aplicarmos ele não obtivemos pontos de interesse para as imagens 5A e 5B. Dessa forma, decidimos utilizar o detector do ORB para esse fim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_keys = list(gray_images.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 5.1: Aplicar ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_keypoints = {}\n",
    "orb_descriptors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv.ORB_create()\n",
    "pos = 0\n",
    "while pos < len(image_keys):\n",
    "    img1 = gray_images[image_keys[pos]]\n",
    "    img2 = gray_images[image_keys[pos+1]]\n",
    "        \n",
    "    orb_keypoints[image_keys[pos]], orb_descriptors[image_keys[pos]] = orb.detectAndCompute(img1,None)\n",
    "    orb_keypoints[image_keys[pos+1]], orb_descriptors[image_keys[pos+1]] = orb.detectAndCompute(img2,None)\n",
    "    \n",
    "    pos+=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 5.2: Aplicar BRIEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_keypoints = {}\n",
    "brief_descriptors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv.ORB_create()\n",
    "brief = cv.xfeatures2d.BriefDescriptorExtractor_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "while pos < len(image_keys):\n",
    "    img1 = gray_images[image_keys[pos]]\n",
    "    img2 = gray_images[image_keys[pos+1]]\n",
    "    \n",
    "    brief_keypoints[image_keys[pos]] = orb.detect(img1,None) #star doesn't find interest points for field1 and field2\n",
    "    brief_keypoints[image_keys[pos+1]] = orb.detect(img2,None)\n",
    "    \n",
    "    brief_keypoints[image_keys[pos]], brief_descriptors[image_keys[pos]] = brief.compute(img1, brief_keypoints[image_keys[pos]])\n",
    "    brief_keypoints[image_keys[pos+1]], brief_descriptors[image_keys[pos+1]] = brief.compute(img2, brief_keypoints[image_keys[pos+1]])\n",
    "    \n",
    "    pos +=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 6: Calcular similaridades e selecionar melhores correspondências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_orb_matches = {}\n",
    "orb_matches = {}\n",
    "\n",
    "aux_brief_matches = {}\n",
    "brief_matches = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brute_force = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_levels = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 6.1: Calcular similares e crossCheck matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "while pos < len(image_keys):\n",
    "    key1 = image_keys[pos]\n",
    "    key2 = image_keys[pos+1]\n",
    "    match_key = key1+'_'+key2\n",
    "    \n",
    "    aux_orb_matches[match_key] = brute_force.match(orb_descriptors[key1],orb_descriptors[key2])\n",
    "    aux_orb_matches[match_key] = sorted(aux_orb_matches[match_key], key = lambda x:x.distance)\n",
    "    \n",
    "    aux_brief_matches[match_key] = brute_force.match(brief_descriptors[key1],brief_descriptors[key2])\n",
    "    aux_brief_matches[match_key] = sorted(aux_brief_matches[match_key], key = lambda x:x.distance)\n",
    "    \n",
    "    pos+=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 6.2: Selecionar melhores correspondências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lvl in corr_levels:\n",
    "    for key in aux_orb_matches:\n",
    "        orb_matches['orb_'+key+'_'+str(lvl)] = getCorrespondingMatches(aux_orb_matches[key],lvl)\n",
    "    for key in aux_brief_matches:\n",
    "        brief_matches['brief_'+key+'_'+str(lvl)] = getCorrespondingMatches(aux_brief_matches[key], lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 7: Estimar matrizes de homografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_homography_matrix = {}\n",
    "brief_homography_matrix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ransac_thresholds = list(np.arange(1,10.5,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thre in ransac_thresholds:\n",
    "    for key in orb_matches:\n",
    "        points1 = np.zeros((len(orb_matches[key]), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(orb_matches[key]), 2), dtype=np.float32)\n",
    "\n",
    "        if len(orb_matches[key])>=4:\n",
    "            for i, match in enumerate(orb_matches[key]):\n",
    "                _,subkey1,subkey2,_ = key.split('_')\n",
    "\n",
    "                points1[i, :] = (orb_keypoints[subkey1])[match.queryIdx].pt\n",
    "                points2[i, :] = (orb_keypoints[subkey2])[match.trainIdx].pt\n",
    "\n",
    "            orb_homography_matrix[key+'_'+str(thre)], mask = cv.findHomography(points1, points2, cv.RANSAC, thre)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thre in ransac_thresholds:\n",
    "    for key in brief_matches:\n",
    "        points1 = np.zeros((len(brief_matches[key]), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(brief_matches[key]), 2), dtype=np.float32)\n",
    "\n",
    "        if len(brief_matches[key])>=4:\n",
    "            for i, match in enumerate(brief_matches[key]):\n",
    "                _,subkey1,subkey2,_ = key.split('_')\n",
    "\n",
    "                points1[i, :] = (brief_keypoints[subkey1])[match.queryIdx].pt\n",
    "                points2[i, :] = (brief_keypoints[subkey2])[match.trainIdx].pt\n",
    "\n",
    "            brief_homography_matrix[key+'_'+str(thre)], mask = cv.findHomography(points1, points2, cv.RANSAC,thre)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for key in brief_homography_matrix:\n",
    "#     print(\"{} - key:{}\\n{}\".format(i,key, brief_homography_matrix[key]))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for key in orb_homography_matrix:\n",
    "#     print(\"{} - key:{}\\n{}\".format(i,key, orb_homography_matrix[key]))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 8: Alinhar imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_panoramic_images = {}\n",
    "brief_panoramic_images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in orb_homography_matrix:\n",
    "    _,subkey1,subkey2,_,_ = key.split('_')\n",
    "    \n",
    "    img1 = images[subkey1]\n",
    "    img2 = images[subkey2]\n",
    "    \n",
    "    height, width,deep = img2.shape\n",
    "    orb_panoramic_images[key] = cv.warpPerspective(img1, orb_homography_matrix[key], (width+img1.shape[1], height+img1.shape[0]))\n",
    "    (orb_panoramic_images[key])[:img2.shape[0],:img2.shape[1]] = img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in brief_homography_matrix:\n",
    "    _,subkey1,subkey2,_,_ = key.split('_')\n",
    "    \n",
    "    img1 = images[subkey1]\n",
    "    img2 = images[subkey2]\n",
    "    \n",
    "    height, width,deep = img2.shape\n",
    "    brief_panoramic_images[key] = cv.warpPerspective(img1, brief_homography_matrix[key], (width+img1.shape[1], height+img1.shape[0]))\n",
    "    (brief_panoramic_images[key])[:img2.shape[0],:img2.shape[1]] = img2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 9: Mostrar imagens com linhas de correspondências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 9.1: Correspondências resultantes dos pontos de interesse obtidos pelo ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrespondingLines(images,orb_matches, orb_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 9.2: Correspondências resultantes dos pontos de interesse obtidos pelo BRIEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrespondingLines(images, brief_matches, brief_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 10: Salvar imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 10.1: Salvar imagens panorâmicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in orb_panoramic_images:\n",
    "#     cv.imwrite('output_images/orb/panoramic/'+key+'.jpg', orb_panoramic_images[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in brief_panoramic_images:\n",
    "#     cv.imwrite('output_images/brief/panoramic/'+key+'.jpg', brief_panoramic_images[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passo 10.2: Salvar imagens com linhas de correspondência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveCorrespondingLines(images,orb_matches, orb_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveCorrespondingLines(images,brief_matches, brief_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
